<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Boost | antonym.org]]></title>
  <link href="http://antonym.org//blog/categories/boost/atom.xml" rel="self"/>
  <link href="http://antonym.org//"/>
  <updated>2014-06-16T23:49:50+10:00</updated>
  <id>http://antonym.org//</id>
  <author>
    <name><![CDATA[Gavin Baker]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Threading with Boost - Part IV: Mutex Examples]]></title>
    <link href="http://antonym.org//2012/02/threading-with-boost---part-iv-mutex-examples.html"/>
    <updated>2012-02-22T00:00:00+11:00</updated>
    <id>http://antonym.org//2012/02/threading-with-boost---part-iv-mutex-examples</id>
    <content type="html"><![CDATA[<p>This article continues the series on threading with Boost, by looking in
depth at several sample programs which illustrate different aspects of
mutexes.  We look at the code, and discuss how it is implemented, including
how to avoid common problems.</p>

<!--more-->


<h1>Mutex Examples</h1>

<p>The full source for the sample applications is provided below.  It is
designed to be fully portable, and has been tested on Mac OS X and Linux.
It should work fine on Windows (please send a patch if it doesn&rsquo;t!).</p>

<p>You can browse all the sources in the <a href="http://bitbucket.org/gavinb/boost_samples">Boost Samples repository</a>
for the latest, or use <a href="http://www.selenic.com/mercurial/">Mercurial</a>
to clone your own copy with this shell command:</p>

<pre><code>hg clone http://hg.antonym.org/src/boost_mutex
</code></pre>

<p>Running <code>bjam</code> or <code>make</code> will build all the examples for you, which you can
run from the appropriate subdirectory (depending on your toolchain and
architecture).</p>

<h1>Duelling Threads</h1>

<p>Source module: <a href="http://bitbucket.org/gavinb/boost_samples/src/tip/mutexes/duel.cpp"><code>duel.cpp</code></a></p>

<p>This example shows how threads are sensitive to timing and scheduling.
There is a global counter which starts at 0.  Two threads are created which
both try to modify the same counter; one incrementing and the other
decrementing.  The program runs for a fixed time, then the counter is
printed.</p>

<p>What would you expect to happen?  (It is worth thinking about this before
reading on&hellip;!)  Assuming the threads get equal time to run, you would think
that their effects would cancel each other out &ndash; that the same number of
increments and decrements would occur, and the result would be 0, or at
least very close to 0. Right?</p>

<p>Here&rsquo;s the result of running the test 10 times for 5 seconds each:</p>

<pre><code>Final counter = -1
Final counter = -3
Final counter = 1
Final counter = -1
Final counter = 0
Final counter = 3
Final counter = -57
Final counter = -8
Final counter = 7
Final counter = -5
</code></pre>

<p>Often the result is quite close to zero, but there are several runs which
are significantly above or below 0. And there&rsquo;s one huge outlier where one
of the counter threads (quiz: which one?) was starved of some time while the
other kept working (answer: the incrementing thread, since the counter is
negative, indicating that the decrementing thread was scheduled for longer).</p>

<p>So even on a machine with very few other processes running, even small
variations in timing and scheduling can lead to a significant variation in
the result.</p>

<p>The moral of this story: <em>the interaction between threads is time-sensitive
and non-deterministic</em> (in the absence of synchronisation).</p>

<h1>Trylock with Queueing Threads</h1>

<p>Source module: <a href="http://bitbucket.org/gavinb/boost_samples/src/tip/mutexes/trylock.cpp"><code>trylock.cpp</code></a></p>

<p>This sample illustrates two cooperating threads: a <em>producer</em> which places
work items in a queue, and a <em>consumer</em> which removes work items from the
queue.  The shared queue is protected by a mutex.</p>

<p>Each thread locks the mutex when pushing or pulling work items, to protect
against concurrent access.  Work items are arbitrarily represented by random
numbers.  Each thread holds the lock for a random time delay to simulate
processing.</p>

<p>Since the producer may be holding the lock when the consumer wants to access
the queue (or vice versa), the consumer performs a <code>try_lock</code> on the mutex.
So instead of blocking until the mutex is free (as a regular call to
<code>lock()</code> would), the call fails immediately if the mutex is already locked,
and the thread resumes.  This contention is reported separately, as it is
important to be able to see how often the threads collide.</p>

<p>Each thread prints out the stage it is executing, to enable analysis of the
interaction.</p>

<p>Note that this technique is <strong>not</strong> a good model for real world use &ndash; it is
specifically designed to illustrate one aspect of using mutexes.  For a much
better implementation of the Producer-Consumer pattern, see the article on
Boost Condition variables later in this series.</p>

<p>Takehome message: <em>thread contention for shared resources wastes processor
cycles and can erase performance improvements gained from concurrent code</em>.</p>

<h1>Mutex Locking with Timeout</h1>

<p>Source module: <a href="http://bitbucket.org/gavinb/boost_samples/src/tip/mutexes/timedlock.cpp"><code>timedlock.cpp</code></a></p>

<p>Shows how to use <code>try_lock</code> on a mutex.  A &lsquo;holding&rsquo; thread idles for a
short time, then grabs the mutex and holds it for another short time before
unlocking it.  The second thread is the &lsquo;trying&rsquo; thread, in that it idles
and then <em>tries</em> to acquire a lock on the mutex.  But it specifies a timeout
by calling the <code>timed_lock()</code> method, and can fail if the holding thread
hasn&rsquo;t released the mutex in time.  If it manages to grab the mutex, it
holds it for a short time also.  The idle and holding times are different,
to ensure the threads don&rsquo;t run in lockstep.  Note that <code>unlock()</code> is only
called if the lock succeeds!</p>

<p>Recommended Usage: <em>Use a timeout when trying to lock a mutex if you can
easily do some other useful work, or if you cannot guarantee you can acquire
the resource within a reasonable timeframe.</em></p>

<h1>Recursive Lock</h1>

<p>Source module: <a href="http://bitbucket.org/gavinb/boost_samples/src/tip/mutexes/reclock.cpp"><code>reclock.cpp</code></a></p>

<p>Illustrates recursively locking a mutex.  A singleton class,
ResourceManager, can register and unregister clients.  Since this may be
called from the context of any worker thread, it uses a mutex when accessing
the dictionary of client information.  Since retain/release management also
requires the mutex be held during updates, this serves as an illustration of
recursive locking.  These methods can be called individually, or from within
the register/unregister methods, which also hold the mutex.  For example:</p>

<pre><code>registerClient()                  lockCount = 0
    mMutex.lock()                 lockCount = 1
    retainClient()                lockCount = 1
        mMutex.lock()             lockCount = 2
        mMutex.unlock()           lockCount = 1
    mMutex.unlock()               lockCount = 0
</code></pre>

<p>Npte: <em>Recursive mutexes should be used rarely, if at all. Thinking you Need
a recursive mutex may be a sign that you have a structural problem with the code.</em></p>

<p>Fun fact: <em>The person who implemented recursive mutexes in <code>pthreads</code> did it
as a bit of a joke, just to prove it was possible &ndash; not really intending for
them to be used in real programs.</em></p>

<h1>Deadlock</h1>

<p>Source module: <a href="http://bitbucket.org/gavinb/boost_samples/src/tip/mutexes/deadlock.cpp"><code>deadlock.cpp</code></a></p>

<p>Imagine you have multiple co-operating threads as well as multiple shared
resources.  Each shared resource is dutifully protected by its own mutex.
So provided each thread locks the mutex before accessing the resource,
everything should run smoothly, right? What could possibly go wrong?</p>

<p>A <a href="http://en.wikipedia.org/wiki/Deadlock"><strong>deadlock</strong></a> is what could go
wrong, and appears as if the program has simply hung.  It&rsquo;s relatively easy
for these situations to occur, but it is also relatively easy to prevent
deadlocks with some care and thought.</p>

<p>A deadlock occurs when thread one is holding lock A while waiting for lock
B, which itself is held by thread two which is waiting for lock A.  In other
words, the two threads are both waiting for each other to do something that
can never happen.  An <em>impasse</em>!</p>

<p>Now this example is <em>intentionally</em> written to fall into a deadlock.  So
whatever you do, don&rsquo;t copy the code from this example into your own code.
Study it and make sure you understand <em>why</em> it is wrong.</p>

<p>In the source, thread one wants to lock both mutex A and mutex B, and it
does so in that order.  It will then perform some processing, and unlock the
two mutexes.  Thread two is similar, in that it wants to lock mutex A and
mutex B before it can safely do its work.  However, the poor programmer who
wrote this code hadn&rsquo;t had their second cup of coffee that morning, and
wrote the code such that it first locks mutex B, <em>then</em> mutex A.  No big
deal, right?</p>

<p>The program prints information about its progress, including which thread is
in what state, such as processing, waiting for a lock, and so on.  This
makes it easier for us to analyse what is going on.  (When debugging
multithreaded code, <code>printf</code> is your friend!)</p>

<p>Each thread holds both locks while they work, which can take a random length
of time.  This setup can work ok for a little while, provided the timing is
just right.  This can be seen in the trace below, which shows the normal
output of the program:</p>

<pre><code>ONE idle
TWO idle
ONE wait_A
ONE wait_B
ONE processing
TWO wait_B
ONE unlock_B
ONE unlock_A
ONE idle
TWO wait_A
TWO processing
TWO unlock_A
TWO unlock_B
TWO idle
TWO wait_B
TWO wait_A
TWO processing
ONE wait_A
TWO unlock_A
TWO unlock_B
TWO idle
ONE wait_B
ONE processing
ONE unlock_B
ONE unlock_A
ONE idle
TWO wait_B
TWO wait_A
TWO processing
TWO unlock_A
TWO unlock_B
TWO idle
TWO wait_B
TWO wait_A
TWO processing
TWO unlock_A
TWO unlock_B
TWO idle
...
</code></pre>

<p>but before long at all, the program will invariably hang.  At this point,
you will have to use <code>Control-C</code> (or your platform&rsquo;s equivalent) to kill the
program.  And if you run it a few times, you will notice the same two
patterns always appearing at the end of the trace; either:</p>

<pre><code>...
ONE wait_A
TWO wait_B
ONE wait_B
TWO wait_A
</code></pre>

<p>or:</p>

<pre><code>...
TWO wait_B
ONE wait_A
TWO wait_A
ONE wait_B
</code></pre>

<p>Reflecting back to the trace when everything seemed to be working, we see
that the threads ran smoothly when they managed to lock both locks at once.
But notice how these telltale signs, these skidmarks leading up to the
moment of the crash, show thread one and two locking are always
<em>interleaved</em> when they acquire these locks?</p>

<p>In the first example, thread one acquires mutex A, and before it can lock
mutex B, thread two is scheduled, and comes along and locks mutex B. Then
thread one tries to acquire mutex B and blocks since it is already locked by
thread two, waiting forever, being a patient worker thread.  At this point,
thread two can run again, and tries to acquire mutex A.  Unfortunately, this
lock is held by thread one which is not coming back until it gets mutex B &ndash;
which will never happen.  Because thread two is now blocked forever, waiting
on mutex A.  Your only consolation is that they probably won&rsquo;t consume any
CPU cycles while they remain in this deadlock.  But the program certainly
isn&rsquo;t going anywhere!</p>

<p>So how can this be avoided?  Fortunately, the fix is simple &ndash; always acquire
the mutexes in the same order, and (ideally) release them in the reverse
order.  This fix is an exercise left for the reader, but verify for yourself
that the program will run as long as you like once the locks are acquired in
the same order.</p>

<p>Why does this work?  Because it prevents the possibility of holding a
contested mutex while blocking on another.</p>

<p>Remember: <em>Always acquire multiple mutexes in the same order to avoid
deadlocks.</em></p>

<h1>Exclusive Shared Mutex with Access Semantics</h1>

<p>Source module: <a href="http://bitbucket.org/gavinb/boost_samples/src/tip/mutexes/sharedlock.cpp"><code>sharedlock.cpp</code></a></p>

<p>This example shows how three worker threads can co-operate accessing a
shared resource, where two are reading and the third is writing.  It is
frequently important to manage access in this way, and can drastically
improve performance by increasing concurrency.</p>

<p>If a thread is only reading from a shared resource, it does not require
exclusive access.  Thus many threads can safely obtain read-only access to a
resource.  It is only if a thread needs to write to the resource to update
it that it will need exclusive access, to ensure the data is updated
atomically (as seen by any worker threads) and remain consistent.</p>

<p>Thus by granting multiple threads concurrent read access, concurrency is
increased in proportion to the number of read-only threads.  The only
disadvantage is that any thread wishing to modify the shared resource
(ie. use an exclusive lock) must wait until <em>all</em> reading threads have
finished.  This provides a good incentive to have the scope of the locks as
short as possible, to reduce the time a writing thread would need to wait
for exclusive access.</p>

<p>In this example, A network thread simulates producing data by ading random
numbers to a container.  A display thread shows the data on screen, and a
database thread writes the numbers to a file.  The network thread uses an
exclusive write lock to update the shared data container, while the other
two threads use a shared read-only lock, which means they can both
concurrently access the data safely.  If the network thread tries to access
the data while either of the other two have a read-lock, it has to wait
until all threads release their shared locks.</p>

<p>(Normally a producer-consumer setup such as this would use a Condition
variable to signal when more data is ready, but we&rsquo;re saving that for a
future installment.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Threading with Boost - Part III: Mutexes]]></title>
    <link href="http://antonym.org//2012/02/threading-with-boost---part-iii-mutexes.html"/>
    <updated>2012-02-20T00:00:00+11:00</updated>
    <id>http://antonym.org//2012/02/threading-with-boost---part-iii-mutexes</id>
    <content type="html"><![CDATA[<p>In <a href="/2009/05/threading-with-boost---part-i-creating-threads.html">Part I</a> of this series on <a href="http://boost.org/">Boost</a> threading, we looked at the basics of how to create and run threads using the Boost libraries.  Then we reviewed the main issues encountered with multithreading code in <a href="/2010/01/threading-with-boost---part-ii-threading-challenges.html">Part II: Threading Challenges</a>. One of the biggest challenges is safely managing concurrent access to a resource.  A Mutex provides a way to serialise access to a shared resource, such that only one thread is accessing the data at any given time, to ensure your data is consistent.  In this article, we look at how to create and use Boost mutexes.</p>

<!--more-->


<h1>Mutual exclusion &ndash; Mutex</h1>

<p>One of the main solutions to race conditions and other threading problems is the <em>mutex</em>, short for &ldquo;mutual exclusion&rdquo;.  Think of it as a special kind of lock, which is used to protect shared resources.  A mutex is special because it is guaranteed to be held by <em>at most</em> one thread at a time.  While one thread holds a lock on a mutex, no other thread is able to.  Thus mutexes can be used as a protection mechanism for resources and data, by locking before using shared resources and unlocking when finished. A mutex can be used to avoid race conditions, and implement atomic operations.</p>

<p>If a mutex is not locked by a thread, then any thread can potentially lock it.  Once it is locked, if any other thread attempts to lock it, it will typically block and wait until the mutex is unlocked by the owning thread.  It is also possible to return if the lock fails, or wait for a certain period of time before giving up.  These variations are discussed below.</p>

<h2>Mutex Scope</h2>

<p>A mutex is typically declared alongside the resource it protects.  The mutex should be in the same scope as the resource, or an enclosing scope.  For example, a data member in a C++ object that can potentially be accessed by more than one thread at a time should have a mutex declared alongside.  A class instance (static data member) should have a similarly class-scoped mutex.  The shared resource should never have a greater scope than the mutex, otherwise it may not always be protected.  Let us not speak of the blight known as global variables, surely a symptom of deficient design.</p>

<p>In simple cases, a single mutex per object may be sufficient, but this is totally dependent on the algorithms used.  If there are multiple resources being accessed independently, having a single mutex may lead to too much contention.  In this case, a separate mutex for each resource may be more efficient.  Only analysis and performance tests will reveal the more efficient choice.</p>

<p>Any time the resource is accessed (whether for reading or writing), the mutex should <em>always</em> be locked before use, and unlocked after the operation.</p>

<h2>Types of Mutexes</h2>

<p>There are four flavours of mutexes available in Boost, depending on your requirements.  Each are described in detail below, and feature a sample program illustrating its use.  The full source code is available for browsing, downloading or even cloning from my Boost Theading Examples Mercurial repository on <a href="http://www.bitbucket.org/gavinb/boost_examples/">BitBucket</a>.</p>

<h3>Regular Mutex</h3>

<p>The simplest form of mutex is a regular <code>boost::mutex</code>.  You lock and unlock it, and only one thread can lock the mutex at a time.  Any thread that calls <code>lock()</code> on a mutex held by another thread will block indefinitely (an important factor when considering synchronisation).  This is worth repeating: calling <code>lock()</code> can block <em>indefinitely</em>.  This may be the desired behaviour, as your thread may need to wait an undetermined time for something else to happen (but this then raises issues of interruption).  It should be obvious then that this must be carefully managed to avoid program hangs, a common symptom of concurrency failure.</p>

<p>```c++</p>

<pre><code>boost::mutex work_queue_mutex;
queue&lt;item&gt; work_queue;

// ...

work_queue_mutex.lock();

auto work_item = work_queue.pop();

work_queue_mutex.unlock();
</code></pre>

<p>```</p>

<p>Regular mutexes also have a <code>try_lock()</code> method, which will return immediately with a failure status if the mutex cannot locked.  Since most threads run in a loop, it would be terribly inefficient to try to lock a mutex and then continue around the loop without performing anything else, especially when it is unlikely that the mutex will become immediately available.  In these cases, it <em>may</em> be appropriate to do a short sleep (but see below for a better solution).  Even then, this may be a sign that your algorithm needs improvement.  Unfortunately, there are no one-size-fits-all solutions.</p>

<p>```c++</p>

<pre><code>boost::mutex work_queue_mutex;
queue&lt;item&gt; work_queue;

// ...

if (work_queue_mutex.try_lock())
{
    auto work_item = work_queue.pop();

    work_queue_mutex.unlock();
}
else
{
    // Do something else...
}
</code></pre>

<p>```</p>

<h3>Timed Mutexes</h3>

<p>The <code>boost::timed_mutex</code> class is a subtype of <code>boost::mutex</code>, which adds the ability to specify a timeout.  For example, you may wish to try to lock the mutex but give up after a certain time if you cannot obtain a lock.  This takes either an absolute time, or a relative time.  If the mutex cannot be obtained within the time specified, the call will return false and the mutex is not held.  If the mutex is locked within the timeout period, it returns true.</p>

<p>```c++</p>

<pre><code>boost::mutex work_queue_mutex;
queue&lt;item&gt; work_queue;
TimeDuration mutex_timeout;

// ...

if (work_queue_mutex.timed_lock(mutex_timeout))
{
    auto work_item = work_queue.pop();

    work_queue_mutex.unlock();
}
</code></pre>

<p>```</p>

<h3>Recursive Mutexes</h3>

<p>Normally a mutex is locked only once, then unlocked.  Depending on the structure of your application, there may be times when it would be useful to be able to lock a mutex multiple times <em>on the one thread</em> (in very special circumstances, such as nested method calls).  For example, you have two (or more) methods which may be called independently, and another method which itself calls one of the other two.  If they all need the mutex held to function safely, this can cause complications when determining when the mutex needs to be locked and released.  However, by using a recursive mutex, it can be locked as many times as necessary, provided it is unlocked the same number of times.  Thus all these methods can be called individually, as they all lock the resources, and in a nested fashion, as the mutex can be locked multiple times.  Provided the mutex is unlocked the same number of times (which is a matter of care and thought), the mutex will be correctly released by the end of the nested operation.</p>

<h3>Shared Mutexes</h3>

<p>Some concurrency scenarios involve having one writer and many readers.  For example, one thread may be downloading data from the network, while another thread is displaying the data on the screen, and a third thread is saving the data to a database.  So the downloading thread will be locking for writing, and the other two only for reading.  There is therefore no reason why the display thread and the database thread (which both only read the shared resource) need to exclude the other; concurrent reading is perfectly safe.  It is only if the network updating thread needs to write to the shared data that the other two need to be locked out.  This process is shown below:</p>

<!-- @todo Insert diagram here -->


<p>Any time the reading threads need to read the resource, they obtain a read-lock.  This allows other read-locks to successfully access the resource, but will prevent a write-lock (since you don&rsquo;t want updates to occur while someone else is reading &ndash; the data must be consistent).  When an update comes in on the networking thread, it must wait until the readers have finished before it can obtain a write-lock.  This will prevent any readers from accessing the resource for the duration of the update.  In this way, concurrent access is increased, contention is reduced, and the resource is always consistent.</p>

<!-- @todo Insert diagram here -->


<p>One of the example programs shows this in action.</p>

<p>```c++</p>

<pre><code>boost::mutex work_queue_mutex;
queue&lt;item&gt; work_queue;

// Reader thread

work_queue_mutex.read_lock();

auto work_item = work_queue.head();

work_queue_mutex.unlock();

// Writer thread

work_queue_mutex.write_lock();

auto work_item = work_queue.push(new_item);

work_queue_mutex.unlock();
</code></pre>

<p>```</p>

<h3>Summary</h3>

<p>The table below summarises the difference mutex types, and their main distinguishing features:</p>

<table>
    <tr>
        <td><tt>boost::mutex</tt></td>
        <td>Normal mutex, most commonly used, blocking lock</td>
        <td>has <tt>lock()</tt> and <tt>try_lock()</tt></td>
    </tr>
    <tr>
        <td><tt>boost::timed_mutex</tt></td>
        <td>Adds ability to timeout waiting for a lock</td>
        <td>adds <tt>timed_lock()</tt> to wait for lock with a timeout</td>
    </tr>
    <tr>
        <td><tt>boost::recursive_mutex</tt></td>
        <td>`lock()` can be called multiple times on the *same* thread</td>
    </tr>
    <tr>
        <td><tt>boost::shared_mutex</tt></td><td>lock can be upgraded to allow multiple readers or a single writer</td><td>can upgrade locks (R->W)</td>
    </tr>
</table>


<h2>Lock duration</h2>

<p>It is important that the mutex only be locked for the shortest possible time to ensure data integrity is maintained.  If a lock is held for too long, it may cause other threads to wait excessively, thus stalling processing and negating the benefits of concurrent processing.  Getting the granularity of threading right takes experience and judgement, so learning by studying existing multithreaded code is an excellent way to pick up design patterns.</p>

<p>The next article shows some timing diagrams which illustrate lock contention in simple apps, so you can see just how much time a thread is waiting.</p>

<h2>Lock/Unlock Pairing</h2>

<p>If a mutex is not released due to a logical error (such as an uncaught exception), this may cause the program to lock up or behave strangely, and is probably not recoverable.  Thus it is vitally important that all lock/unlock operations appear in pairs.</p>

<p>To protect against this class of problem, the <code>lock_guard</code> object was introduced.  It locks the mutex for you in its constructor, and unlocks it in the destructor.  (This safety-conscious approach is known as <a href="http://en.wikipedia.org/wiki/RAII">RAII</a>, or Resource Acquisition Is Initialisation, whereby the lifecycle of a resource is tied to the lifecycle of an owning object).</p>

<p>This simple example code shows potentially unsafe method implementation.</p>

<p>```c++</p>

<pre><code>void writeTotalsToDatabase() throw (myapp::SQLException)
{
    // does SQL stuff, and throws on error
}

unsigned applyTotals(unsigned count)
{
    mTotalsMutex.lock();

    mTotals.globalCount += count;

    writeTotalsToDatabase();

    mTotalsMutex.unlock();
}
</code></pre>

<p>```</p>

<p>This code looks at first glance to be safe.  It simply makes the updating and saving of the totals atomic, right?  Well, yes &ndash; except if there&rsquo;s an exception thrown by <code>writeTotalsToDatabase()</code>.  If that happens, the normal flow of execution goes out the window, and the program unwinds the stack in search of a suitable <code>catch</code> statement.  And the lock that we have acquired will never be released!  This is very dangerous, and can happen in seemingly &ldquo;safe&rdquo; code.  While you could certainly put a <code>try/catch</code> around the database method, a better way is to use a lock guard.</p>

<p>```c++</p>

<pre><code>unsigned applyTotals(unsigned count)
{
    boost::lock_guard    totalsLock(mTotalsMutex);

    mTotals.globalCount += count;

    writeTotalsToDatabase();
}
</code></pre>

<p>```</p>

<p>The <code>totalsLock</code> will acquire (or wait to acquire) the mutex at the start, and when it goes out of scope at the end of the method, it will unlock the mutex.  If an exception is thrown, the lock&rsquo;s destructor will be invoked as part of unwinding the stack, and there the mutex will be safely released.  This ensures the operation is atomic, and can safely handle error conditions.</p>

<h2>Avoiding Deadlocks</h2>

<p>So far, we&rsquo;ve looked at using one mutex at a time.  But a non-trivial application may involve many threads, and even more mutexes.  And what happens when there is contention for mutexes between threads?  There is a fatal error known as a deadlock, and in its simplest form, two threads are holding a resource each, which the other also wants.  Since neither will release one until it gets the other, the threads get stuck waiting forever (or at least until the user gets sick of waiting and kills the process).</p>

<h2>Shared Object with One Mutex</h2>

<p>Imagine you have a shared object, such as a singleton, that may be accessed by multiple threads in the system.  A simple solution to ensure consistent data is returned is to give the object a mutex, and use a lock guard in every method.  For example:</p>

<p>```c++</p>

<pre><code>class Baz
{
    public:
        Baz();
        void accumulate(const Foo&amp; foo)
        {
            boost::lock_guard lock(m_mutex);

            m_total += foo.value();
        }
        float getTotal() const
        {
            boost::lock_guard lock(m_mutex);

            return m_total;
        }
    private:
        boost::mutex m_mutex;
        double      m_total;
};
</code></pre>

<p>```</p>

<h2>A note on performance</h2>

<p>Under Linux, the Boost threads implementation uses pthreads and pthread mutexes.  These may use a spinlock at the kernel level, for optimal performance.</p>

<p>Under Windows, a real &ldquo;Mutex&rdquo; object at the operating system level is actually a fairly expensive, heavyweight and slow construct (relatively speaking), mainly intended for intra-process synchronisation.  If you only need serialisation within your process, a &ldquo;critical section&rdquo; is a far better choice, as it is significantly faster.  Fortunately, Boost uses a CritSec for process-level mutexes.  If you need inter-process mutexes, look at the <a href="http://boost.org/">Boost IPC library</a>.</p>

<h2>Other types of Mutexes</h2>

<p>There are a few different flavours of Mutexes, which can be used in more sophisticated scenarios. These are beyond the scope of this article (which is already long enough!).</p>

<h2>Up Next</h2>

<p>The next article will focus on <a href="/2012/02/threading-with-boost---part-iv-mutex-examples.html">a series of examples showing the use of different mutex types</a>. The source has been published on BitBucket in my <a href="http://bitbucket.org/gavinb/boost_samples/">Boost Sample Code</a> repository.  Feel free to use, fork, and share!</p>
]]></content>
  </entry>
  
</feed>
